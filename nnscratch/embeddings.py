# -*- coding: utf-8 -*-
"""embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHw-dKpfgTBqwFFFbgQaktF_qOUT9yfl
"""

import cupy as cp
import numpy as np

class EmbeddingLayer:
    def __init__(self, E: cp.ndarray, lr: float=0.01, trainable: bool=True):
        """
        E: pretrained embedding matrix of shape (D, V)
        lr: learning rate for fine-tuning
        trainable: whether to update E during backprop. If True, update embeddings during backprop; otherwise keep fixed.
        """
        self.E = E
        self.lr = lr
        self.trainable = trainable
        self.last_x = None  # Cache of the most recent one-hot matrix (V × n) so we know which column of E to update during backprop.

    def forward(self, x_onehot: cp.ndarray) -> cp.ndarray:
        """
        x_onehot: concatenated one-hot vector, shape = (V * n, 1)
        returns: stacked embedding vector, shape = (D * n, 1)
        """
        # Vocabulary size V, embedding dimension D
        V = self.E.shape[1]
        D = self.E.shape[0]

        # Number of tokens in the window total length = V * n, so n = total_length // V
        n = x_onehot.shape[0] // V

        # Reshape into a (V × n) one-hot matrix, each column is one token
        Xmat = x_onehot.reshape(V, n)
        self.last_x = Xmat

        # Lookup: E @ Xmat → (D × n) gives embeddings for each token
        emb = self.E @ Xmat             # (D, n)
        return emb.reshape(D * n, 1)    # (D*n, 1)

    def backward(self, grad_emb: cp.ndarray):
        """
        Backward pass: fine-tune embedding matrix columns based on upstream gradient.

        Args:
          grad_emb (cp.ndarray): Gradient w.r.t. the forward output,
                                 shape = (D * n, 1).

        """
        # If embeddings are frozen, do nothing
        if not self.trainable:
            return None

        # Embedding dimension D and window size n
        D = self.E.shape[0]
        n = grad_emb.shape[0] // D
        # Reshape gradient to (D, n), matching emb = E @ Xmat
        G = grad_emb.reshape(D, n)

        for j in range(n):
            # Extract the one-hot column for token j (shape V,)
            onehot_col = self.last_x[:, j]
            # Find the index of the active (1) entry → actual token ID
            idx = int(cp.argmax(onehot_col))
            # Extract gradient slice corresponding to that token (shape D×1)
            grad_col = G[:, j:j+1]
            # Update only that column of E: gradient descent step
            self.E[:, idx:idx+1] -= self.lr * grad_col

        return None