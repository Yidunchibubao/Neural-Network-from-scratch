# -*- coding: utf-8 -*-
"""layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCkR6ai4Im6rI7Pk7s-x6O_nR4e-3lnt
"""

from . import np


class LinearLayer:
    def __init__(self, input_size, output_size):
        """
        Affine layer computing a^(k) = W^(k) · h^(k-1) + b^(k)
        """
        self.input_size  = input_size
        self.output_size = output_size
        self.W = np.random.randn(output_size, input_size) * 0.01  # W^(k)
        self.b = np.zeros((output_size, 1))                       # b^(k)#   - initialized to zeros so that the network does not favor any direction initially.
        self.h_prev = None                                        # stores h^(k-1)

    def forward(self, h_prev):
        """
        h_prev: h^(k-1), shape = (input_size, batch)
        returns a^(k), shape = (output_size, batch)
        """
        self.h_prev = h_prev
        return self.W @ h_prev + self.b

    def backward(self, grad_a, lr):
        """
        grad_a: ∇_{a^(k)} L, shape = (output_size, batch)
        lr:    learning rate

        returns ∇_{h^(k-1)} L, shape = (input_size, batch)
        and updates W^(k), b^(k)
        """
        # ∇_{W^(k)} = grad_a @ h_prev.T
        grad_W = grad_a @ self.h_prev.T
        # ∇_{b^(k)} = sum over batch
        grad_b = np.sum(grad_a, axis=1, keepdims=True)
        # ∇_{h^(k-1)} = W^(k).T @ grad_a
        grad_h_prev = self.W.T @ grad_a

        # update parameters
        self.W -= lr * grad_W
        self.b -= lr * grad_b

        return grad_h_prev

class ActivationFunction:
    def __init__(self, name):
        """
        Supports 'relu', 'sigmoid', 'tanh', 'softmax' (with cross-entropy).
        """
        name = name.lower()
        if name == "relu":
            self.func = self._relu
            self.grad = self._grad_relu
        elif name == "sigmoid":
            self.func = self._sigmoid
            self.grad = self._grad_sigmoid
        elif name == "tanh":
            self.func = self._tanh
            self.grad = self._grad_tanh
        elif name == "softmax":
            self.func = self._softmax
            # under cross-entropy, ∇_{a^(L+1)} = (y_pred - y_true)
            self.grad = lambda dh, a: dh
        else:
            raise ValueError(f"Unsupported activation '{name}'")

        self.a = None  # stores pre-activation a^(k)

    def forward(self, a):
        """
        a: pre-activation a^(k), shape = (size, batch)
        returns h = g(a), shape = (size, batch)
        """
        self.a = a
        return self.func(a)

    def backward(self, grad_h):
        """
        grad_h: ∇_{h^(k)} L, shape = (size, batch)
        returns ∇_{a^(k)} L = grad_h ⊙ g'(a)
        """
        return self.grad(grad_h, self.a)

    # --- nonlinearities ---
    def _relu(self,    x): return np.maximum(0, x)
    def _sigmoid(self, x): return 1 / (1 + np.exp(-x))
    def _tanh(self,    x): return np.tanh(x)
    def _softmax(self, x):
        ex = np.exp(x - np.max(x, axis=0, keepdims=True))
        return ex / np.sum(ex, axis=0, keepdims=True)

    # --- derivatives ---
    def _grad_relu(self,    dh, a): return dh * (a > 0)
    def _grad_sigmoid(self, dh, a):
        s = 1 / (1 + np.exp(-a))
        return dh * s * (1 - s)
    def _grad_tanh(self,    dh, a):
        t = np.tanh(a)
        return dh * (1 - t**2)
