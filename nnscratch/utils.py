# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wndeoYc03iChnFhw-xgNr5psU3nt0_sg
"""

import conllu
from . import cp, np


def load_conllu(filename):
  sentences = []
  tags = []
  with open(filename, "rt", encoding="utf-8") as f:
    for sentence in conllu.parse(f.read()):
      words = []
      upos_tags = []
      for token in sentence:
        if isinstance(token["id"], int):
          if token["upos"] not in {"PUNCT", "NUM", "SYM", "X"}:
            words.append(token["form"].lower())
            upos_tags.append(token["upos"])
      if words:  # skip empty sentences
        sentences.append(words)
        tags.append(upos_tags)
  return sentences, tags

def build_vocab(sentences, tags):
  word_set = set()
  tag_set = set()
  for sent in sentences:
    word_set.update(sent)
  for tag_seq in tags:
    tag_set.update(tag_seq)

  vocab_list = ["<PAD>", "<UNK>"] + sorted(word_set)
  tag_list = ["<PAD>"] + sorted(tag_set)
  word2id = {word: idx for idx, word in enumerate(vocab_list)}
  tag2id = {tag: idx for idx, tag in enumerate(tag_list)}

  return word2id, tag2id

# build a one-hot vector for a word, the position of the word id is 1, other positions are 0
# the dimension of one-hot vector is the size of vocabulary
def one_hot_vector(index, size):
  vec = cp.zeros((size, 1))
  if 0 <= index < size:
    vec[index, 0] = 1
  return vec

# convert one sentence to a list of one-hot vectors
def sentence_to_onehot(sent, word2id):
  vocab_size = len(word2id)
  unk_id = word2id.get("<UNK>")
  onehot_vectors = []
  for word in sent:
    idx = word2id.get(word, unk_id)
    onehot_vectors.append(one_hot_vector(idx, vocab_size))
  return onehot_vectors

# convert a sequence of tags (one sentence) to a list of one-hot vector
def tags_to_onehot(tag_seq, tag2id):
  tag_size = len(tag2id)
  pad_id = tag2id.get("<PAD>")
  onehot_vectors = []
  for tag in tag_seq:
    idx = tag2id.get(tag, pad_id)
    onehot_vectors.append(one_hot_vector(idx, tag_size))
  return onehot_vectors


def build_windowed_dataset(sentences, tags, word2id, tag2id, window_size=1):
  """
  build examples for training：
  input for a word: concatenated one-hot vector of [previous word, current word, next word]
  label for a word: corresponding POS tag of current word (one-hot vector)

  return：
  - inputs: list of cp.array of shape (vocab_size * window_size*2+1, 1)
  - labels: list of cp.array of shape (num_tags, 1)
  """
  vocab_size = len(word2id)
  num_tags = len(tag2id)
  pad_token = "<PAD>"
  unk_token = "<UNK>"

  pad_id = word2id[pad_token]
  unk_id = word2id[unk_token]

  inputs = []
  labels = []

  for sent, tag_seq in zip(sentences, tags):
    # add pad token at the start and end of each sentence to build complete window
    padded_sent = [pad_token]*window_size + sent + [pad_token]*window_size
    padded_tags = [pad_token]*window_size + tag_seq + [pad_token]*window_size

    for i in range(window_size, len(padded_sent) - window_size):
      window = padded_sent[i-window_size:i+window_size+1]
      tag = padded_tags[i]

      # concatenate one-hot vectors
      onehot_window = [
        one_hot_vector(word2id.get(w, unk_id), vocab_size) for w in window
      ]
      x = cp.vstack(onehot_window)  # shape: (vocab_size * (2*window_size + 1), 1)

      # build one-hot vector for corresponding POS tag
      y = one_hot_vector(tag2id.get(tag, tag2id["<PAD>"]), num_tags)

      inputs.append(x)
      labels.append(y)

  return inputs, labels